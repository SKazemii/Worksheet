{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# keras imports\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras import preprocessing, callbacks \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Flatten\n",
    "\n",
    "# import seaborn as sns\n",
    "# import tsfel\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys, logging\n",
    "from PIL import Image\n",
    "from pathlib import Path as Pathlb\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "from MLPackage import config as cfg\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m[12/03/2021 01:00:16 PM ]-\u001b[38;5;226m[2DCNN_ipynb @37]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tImporting libraries....\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "project_dir = os.getcwd()[:-5]\n",
    "log_path = os.path.join(project_dir, 'logs')\n",
    "temp_dir = os.path.join(project_dir, \"temp\")\n",
    "\n",
    "Pathlb(log_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def create_logger(level):\n",
    "    loggerName = \"2DCNN_ipynb\"\n",
    "    Pathlb(log_path).mkdir(parents=True, exist_ok=True)\n",
    "    grey = '\\x1b[38;21m'\n",
    "    blue = '\\x1b[38;5;39m'\n",
    "    yellow = '\\x1b[38;5;226m'\n",
    "    red = '\\x1b[38;5;196m'\n",
    "    bold_red = '\\x1b[31;1m'\n",
    "    reset = '\\x1b[0m'\n",
    "\n",
    "    logger = logging.getLogger(loggerName)\n",
    "    logger.setLevel(level)\n",
    "    formatter_colored = logging.Formatter(blue + '[%(asctime)s]-' + yellow + '[%(name)s @%(lineno)d]' + reset + blue + '-[%(levelname)s]' + reset + bold_red + '\\t\\t%(message)s' + reset, datefmt='%m/%d/%Y %I:%M:%S %p ')\n",
    "    formatter = logging.Formatter('[%(asctime)s]-[%(name)s @%(lineno)d]-[%(levelname)s]\\t\\t%(message)s', datefmt='%m/%d/%Y %I:%M:%S %p ')\n",
    "    file_handler = logging.FileHandler( os.path.join(log_path, loggerName + '_loger.log'), mode = 'w')\n",
    "    file_handler.setLevel(level)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    stream_handler.setFormatter(formatter_colored)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "    return logger\n",
    "logger = create_logger(logging.DEBUG)\n",
    "\n",
    "\n",
    "logger.info(\"Importing libraries....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m[12/03/2021 01:00:16 PM ]-\u001b[38;5;226m[2DCNN_ipynb @2]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tLoading efficientnet.EfficientNetB0 model...\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:18 PM ]-\u001b[38;5;226m[2DCNN_ipynb @4]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tSuccessfully loaded base model and model...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Loading { cfg.configs['CNN']['base_model'] } model...\")\n",
    "    base_model = eval(\"tf.keras.applications.\" + cfg.configs[\"CNN\"][\"base_model\"] + \"(weights=cfg.configs['CNN']['weights'], include_top=cfg.configs['CNN']['include_top'])\")\n",
    "    logger.info(\"Successfully loaded base model and model...\")\n",
    "\n",
    "except Exception as e: \n",
    "    \n",
    "    base_model = None\n",
    "    logger.error(\"The base model could NOT be loaded correctly!!!\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "CNN_name = cfg.configs['CNN'][\"base_model\"].split(\".\")[0]\n",
    "\n",
    "input = tf.keras.layers.Input(shape=cfg.configs['CNN'][\"image_size\"], dtype = tf.float64, name=\"original_img\")\n",
    "x = tf.cast(input, tf.float32)\n",
    "x = eval(\"tf.keras.applications.\" + CNN_name + \".preprocess_input(x)\")\n",
    "x = base_model(x)\n",
    "output = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"efficientnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 120, 200, 3)]     0         \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 120, 200, 3)       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 1280)             0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(input, output, name=CNN_name)\n",
    "model.summary() \n",
    "tf.keras.utils.plot_model(model, to_file=CNN_name + \".png\", show_shapes=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m[12/03/2021 01:00:19 PM ]-\u001b[38;5;226m[2DCNN_ipynb @3]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tprefeature shape: (2851, 60, 40, 10)\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:19 PM ]-\u001b[38;5;226m[2DCNN_ipynb @7]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tbatch_size: 32\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Loading_path = os.path.join(project_dir, 'Datasets', 'prefeatures.npy')\n",
    "prefeatures = np.load(Loading_path)\n",
    "logger.info(\"prefeature shape: {}\".format(prefeatures.shape))\n",
    "\n",
    "\n",
    "# #CD, PTI, Tmax, Tmin, P50, P60, P70, P80, P90, P100\n",
    "logger.info(\"batch_size: {}\".format(cfg.configs['CNN'][\"batch_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatenning Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = list()\n",
    "for sample in prefeatures:\n",
    "    sample = sample.transpose((2, 0, 1))\n",
    "\n",
    "    total_image = sample[0,:,:]\n",
    "    total_image1 = sample[5,:,:]\n",
    "\n",
    "    for i in range(1,5):\n",
    "        total_image = np.concatenate((total_image, sample[i,:,:]), axis=1)\n",
    "        total_image1 = np.concatenate((total_image1, sample[i+5,:,:]), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_image = np.concatenate((total_image, total_image1), axis=0)\n",
    "    total_image = total_image[:,:, np.newaxis]\n",
    "    total_image = np.concatenate((total_image, total_image, total_image), axis=2)\n",
    "\n",
    "    # print(total_image.shape)\n",
    "    images.append(total_image)\n",
    "\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    # plt.imshow( total_image)\n",
    "    # plt.show()\n",
    "\n",
    "    # print(type(total_image))\n",
    "    # print(total_image.dtype)\n",
    "\n",
    "\n",
    "\n",
    "    # result = model(total_image)\n",
    "    # print(result)\n",
    "\n",
    "    # break\n",
    "images =np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m[12/03/2021 01:00:23 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 0\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:27 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 128\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:31 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 256\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:35 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 384\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:39 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 512\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:43 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 640\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:47 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 768\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:50 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 896\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:53 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1024\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:00:57 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1152\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:00 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1280\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:04 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1408\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:07 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1536\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:11 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1664\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:15 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1792\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:19 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 1920\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:22 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2048\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:26 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2176\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:30 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2304\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:33 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2432\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:37 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2560\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:40 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2688\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:44 PM ]-\u001b[38;5;226m[2DCNN_ipynb @9]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t ->>> completed images: 2816\u001b[0m\n",
      "\u001b[38;5;39m[12/03/2021 01:01:44 PM ]-\u001b[38;5;226m[2DCNN_ipynb @15]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\t(2851, 1280)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Deep_features = np.zeros((1, model.layers[-1].output_shape[1]))\n",
    "for image in range(0, images.shape[0], cfg.configs['CNN'][\"batch_size\"]):\n",
    "    pic = images[image:image+cfg.configs[\"CNN\"][\"batch_size\"],:,:,:]\n",
    "\n",
    "    feature = model(pic)\n",
    "\n",
    "    Deep_features = np.append(Deep_features, feature, axis=0)\n",
    "    if image % 128 == 0:\n",
    "        logger.info(\" ->>> completed images: \" + str(image))\n",
    "\n",
    "    # if image > 128:\n",
    "    #     break\n",
    "\n",
    "Deep_features = Deep_features[1:, :]\n",
    "logger.info(Deep_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m[12/03/2021 01:01:44 PM ]-\u001b[38;5;226m[2DCNN_ipynb @4]\u001b[0m\u001b[38;5;39m-[INFO]\u001b[0m\u001b[31;1m\t\tMetadata shape: (2851, 7)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "meta_path = os.path.join(project_dir, 'Datasets', 'metadatalist.npy')\n",
    "metadata = np.load(meta_path)\n",
    "\n",
    "logger.info(\"Metadata shape: {}\".format(metadata.shape))\n",
    "time = int(timeit.default_timer() * 1_000_000)\n",
    "\n",
    "\n",
    "saving_path = os.path.join(temp_dir, CNN_name+'_features.xlsx')\n",
    "columnsName = [CNN_name+\"_\"+str(i) for i in range(Deep_features.shape[1])]  + cfg.label\n",
    "Deep_features = np.concatenate((Deep_features, metadata[:Deep_features.shape[0], 0:2]), axis=1)\n",
    "\n",
    "try:\n",
    "    pd.DataFrame(Deep_features, columns=columnsName).to_excel(saving_path)\n",
    "except:\n",
    "    pd.DataFrame(Deep_features, columns=columnsName).to_excel(os.path.join(temp_dir, CNN_name+'_features_'+str(time)+'.xlsx'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d420073edfc19a3b1ff3b429a894eba8b52a8b645a9b23b28e961e231b9db723"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
