{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# keras imports\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.keras import preprocessing, callbacks \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Flatten\n",
    "\n",
    "# import seaborn as sns\n",
    "# import tsfel\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys, logging, h5py\n",
    "from PIL import Image\n",
    "from pathlib import Path as Pathlb\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from scipy import signal\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))\n",
    "from MLPackage import Features as feat\n",
    "from MLPackage import config as cfg\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd()[:-5]\n",
    "log_path = os.path.join(project_dir, 'logs')\n",
    "temp_dir = os.path.join(project_dir, \"temp\")\n",
    "\n",
    "Pathlb(log_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "def create_logger(level):\n",
    "    loggerName = \"2DCNN_ipynb\"\n",
    "    Pathlb(log_path).mkdir(parents=True, exist_ok=True)\n",
    "    grey = '\\x1b[38;21m'\n",
    "    blue = '\\x1b[38;5;39m'\n",
    "    yellow = '\\x1b[38;5;226m'\n",
    "    red = '\\x1b[38;5;196m'\n",
    "    bold_red = '\\x1b[31;1m'\n",
    "    reset = '\\x1b[0m'\n",
    "\n",
    "    logger = logging.getLogger(loggerName)\n",
    "    logger.setLevel(level)\n",
    "    formatter_colored = logging.Formatter(blue + '[%(asctime)s]-' + yellow + '[%(name)s @%(lineno)d]' + reset + blue + '-[%(levelname)s]' + reset + bold_red + '\\t\\t%(message)s' + reset, datefmt='%m/%d/%Y %I:%M:%S %p ')\n",
    "    formatter = logging.Formatter('[%(asctime)s]-[%(name)s @%(lineno)d]-[%(levelname)s]\\t\\t%(message)s', datefmt='%m/%d/%Y %I:%M:%S %p ')\n",
    "    file_handler = logging.FileHandler( os.path.join(log_path, loggerName + '_loger.log'), mode = 'w')\n",
    "    file_handler.setLevel(level)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    stream_handler.setFormatter(formatter_colored)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "    return logger\n",
    "logger = create_logger(logging.DEBUG)\n",
    "\n",
    "\n",
    "logger.info(\"Importing libraries....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"[INFO] reading dataset....\")\n",
    "with h5py.File(cfg.configs[\"paths\"][\"stepscan_dataset\"], \"r\") as hdf:\n",
    "    barefoots = hdf.get(\"/barefoot/data\")[:]\n",
    "    metadata = hdf.get(\"/barefoot/metadata\")[:]\n",
    "\n",
    "\n",
    "data = barefoots.transpose(0,2,3,1)\n",
    "\n",
    "logger.info(f\"barefoots.shape: {data.shape}\")\n",
    "logger.info(f\"metadata.shape: {metadata.shape}\")\n",
    "plt.imshow(data[1,:,:,:].sum(axis=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "labels = list()\n",
    "\n",
    "for label, sample in zip(metadata, data):\n",
    "    # print(np.shape(sample))\n",
    "    # print(np.max(sample))\n",
    "    try:\n",
    "        B = sample.sum(axis=1).sum(axis=0)\n",
    "        A = np.trim_zeros(sample.sum(axis=1).sum(axis=0))\n",
    "        aa = np.where(B == A[0])\n",
    "        bb = np.where(B == A[-1])\n",
    "        # print(aa[0][0])\n",
    "        # print(bb[0][0])\n",
    "        # print(np.trim_zeros(sample.sum(axis=1).sum(axis=0)))\n",
    "\n",
    "        if aa[0][0]<bb[0][0]:\n",
    "            features.append(feat.prefeatures(sample[10:70, 10:50, aa[0][0]:bb[0][0]]))\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(aa[0][0],bb[0][0])\n",
    "            k=sample\n",
    "            l=label\n",
    "    except:\n",
    "        continue\n",
    "    # labels.append(label)\n",
    "    # break\n",
    "    # f = np.trim_zeros(sample.sum(axis=1).sum(axis=0),'f').shape[0]\n",
    "    # b = np.trim_zeros(sample.sum(axis=1).sum(axis=0),'b').shape[0]\n",
    "    # print(b, f,)\n",
    "\n",
    "    # temp = np.zeros(sample[:,:,b:f].shape)\n",
    "    # temp[sample[:,:,b:f] > 5] = 1\n",
    "    # CD = np.sum(temp, axis=2)\n",
    "\n",
    "    \n",
    "        \n",
    "    # # plt.imshow(sample[:,:,100])\n",
    "    # print(np.max(sample[:,:,100]))\n",
    "    # # break\n",
    "    \n",
    "\n",
    "\n",
    "logger.info(f\"len prefeatures: {len(features)}\")\n",
    "logger.info(f\"prefeatures.shape: {features[0].shape}\")\n",
    "logger.info(f\"labels.shape: {labels[0].shape}\")\n",
    "\n",
    "np.save(os.path.join(temp_dir, 'prefeatures-SS.npy'), features)\n",
    "np.save(os.path.join(temp_dir, 'metadata.npy'), labels)\n",
    "\n",
    "# plt.imshow(CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading_path = os.path.join(temp_dir, 'prefeatures-SS.npy')\n",
    "prefeatures = np.load(Loading_path)\n",
    "logger.info(\"prefeature shape: {}\".format(prefeatures.shape))\n",
    "\n",
    "Loading_path = os.path.join(temp_dir, 'metadata.npy')\n",
    "metadata = np.load(Loading_path)\n",
    "logger.info(\"prefeature shape: {}\".format(metadata.shape))\n",
    "\n",
    "# #CD, PTI, Tmax, Tmin, P50, P60, P70, P80, P90, P100\n",
    "logger.info(\"batch_size: {}\".format(cfg.configs[\"CNN\"][\"batch_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatenning Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images  = list()\n",
    "for sample in prefeatures:\n",
    "    sample = sample.transpose((2, 0, 1))\n",
    "\n",
    "    total_image = sample[0,:,:]\n",
    "    total_image1 = sample[5,:,:]\n",
    "\n",
    "    for i in range(1,5):\n",
    "        total_image = np.concatenate((total_image, sample[i,:,:]), axis=1)\n",
    "        total_image1 = np.concatenate((total_image1, sample[i+5,:,:]), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_image = np.concatenate((total_image, total_image1), axis=0)\n",
    "    total_image = total_image[:,:, np.newaxis]\n",
    "    total_image = np.concatenate((total_image, total_image, total_image), axis=2)\n",
    "\n",
    "    images.append(total_image)\n",
    "\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    # plt.imshow( total_image)\n",
    "    # plt.show()\n",
    "\n",
    "    # print(type(total_image))\n",
    "    # print(total_image.dtype)\n",
    "    # print(total_image.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # print(result)\n",
    "\n",
    "    # break\n",
    "images =np.array(images)\n",
    "plt.imshow(images[55,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# indices = metadata[:,0]\n",
    "# depth = len(np.unique(metadata[:,0]))\n",
    "# one_hot_labels = tf.one_hot(indices, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CC=tf.keras.utils.to_categorical( metadata[:,0], num_classes=len(np.unique(metadata[:,0])))\n",
    "# # one_hot_labels==CC\n",
    "# len(np.unique(metadata[:,0]))\n",
    "from sklearn import preprocessing as pre\n",
    "indices = metadata[:,0]\n",
    "le = pre.LabelEncoder()\n",
    "le.fit(indices)\n",
    "le.classes_\n",
    "indices=le.transform(indices)\n",
    "len(np.unique(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(f\"Loading { cfg.configs['CNN']['base_model'] } model...\")\n",
    "    base_model = eval(\"tf.keras.applications.\" + cfg.configs[\"CNN\"][\"base_model\"] + \"(weights=cfg.configs['CNN']['weights'], include_top=cfg.configs['CNN']['include_top'])\")\n",
    "    logger.info(\"Successfully loaded base model and model...\")\n",
    "\n",
    "except Exception as e: \n",
    "    \n",
    "    base_model = None\n",
    "    logger.error(\"The base model could NOT be loaded correctly!!!\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "CNN_name = cfg.configs[\"CNN\"][\"base_model\"].split(\".\")[0]\n",
    "\n",
    "input = tf.keras.layers.Input(shape=cfg.configs[\"CNN\"][\"image_size\"], dtype = tf.float64, name=\"original_img\")\n",
    "x = tf.cast(input, tf.float32)\n",
    "x = eval(\"tf.keras.applications.\" + CNN_name + \".preprocess_input(x)\")\n",
    "x = base_model(x)\n",
    "x = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu', name=\"last_dense\")(x)\n",
    "output = tf.keras.layers.Dense(cfg.configs['CNN']['class_numbers'], name=\"prediction\")(x) # cfg.configs['CNN']['class_numbers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=input, outputs=output, name=cfg.configs['CNN']['base_model'])\n",
    "\n",
    "# Freeze the layers \n",
    "for layer in model.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# for i,layer in enumerate(model.layers):\n",
    "#     print(i,layer.name,layer.trainable)\n",
    "\n",
    "model.summary() \n",
    "\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file=cfg.configs['CNN']['base_model'] + \".png\", show_shapes=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.001), \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "time = int(timeit.timeit())*1_000_000\n",
    "checkpoint = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            cfg.configs[\"CNN\"][\"saving_path\"], save_best_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "        ),\n",
    "        callbacks.EarlyStopping( monitor=\"val_loss\", patience=50, verbose=1),\n",
    "        callbacks.TensorBoard(log_dir='TensorBoard_logs/{time}')\n",
    "        \n",
    "    ]    \n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    images,\n",
    "    indices,\n",
    "    batch_size=cfg.configs[\"CNN\"][\"batch_size\"],\n",
    "    callbacks=[checkpoint],\n",
    "    epochs= cfg.configs[\"CNN\"][\"epochs\"],\n",
    "    validation_split=cfg.configs[\"CNN\"][\"validation_split\"],\n",
    "    verbose=cfg.configs[\"CNN\"][\"verbose\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d420073edfc19a3b1ff3b429a894eba8b52a8b645a9b23b28e961e231b9db723"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
